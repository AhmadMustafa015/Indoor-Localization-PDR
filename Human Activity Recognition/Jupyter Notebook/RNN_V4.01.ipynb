{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "os.environ['TF_ENABLE_CONTROL_FLOW_V2'] = '1'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import std\n",
    "from numpy import mean\n",
    "from numpy import dstack\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import seaborn as sn\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import datetime\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4\n",
      "dtype: int64\n",
      "(5945, 128, 9) (5945, 1)\n"
     ]
    }
   ],
   "source": [
    "num_runs = 1 # Total number of repeats\n",
    "filesname_train = list()\n",
    "#file name of train data\n",
    "filesname_train += ['total_acc_x_train.txt', 'total_acc_y_train.txt', 'total_acc_z_train.txt']\n",
    "filesname_train += ['body_acc_x_train.txt', 'body_acc_x_train.txt', 'body_acc_x_train.txt']\n",
    "filesname_train += ['body_gyro_x_train.txt', 'body_gyro_y_train.txt', 'body_gyro_z_train.txt']\n",
    "dataY_trainingRaw = pd.read_csv('Dataset/train/y_train.txt',header=None, delim_whitespace=True)\n",
    "index1 = dataY_trainingRaw[dataY_trainingRaw[0] == 6].index\n",
    "# indexStanding = dataY_trainingRaw[dataY_trainingRaw[0] == 5].index\n",
    "# indexSitting = dataY_trainingRaw[dataY_trainingRaw[0] == 4].index\n",
    "# #print(index1.shape)\n",
    "# templist = dataY_trainingRaw.index.tolist()\n",
    "# idx = templist.index(5)\n",
    "# templist[idx] = 4\n",
    "# dataY_trainingRaw.index = templist;\n",
    "dataY_trainingRaw.drop(index= index1, inplace = True)\n",
    "dataY_trainingRaw.replace(5,4,inplace = True)\n",
    "print(dataY_trainingRaw.max())\n",
    "dataX_training = list()\n",
    "for name in filesname_train:\n",
    "    frame = pd.read_csv('Dataset/train/Inertial Signals/' + name,header=None, delim_whitespace=True) # load a single file\n",
    "    frame.drop(index= index1, inplace = True)\n",
    "    dataX_training.append(frame.values) # store data as a numpy array\n",
    "# dataX_training is a 3D numpy array (samples[number of raws is any file), time steps(number of readings in a single window ex: 128)\n",
    "#                               ,features[nine features 3-axis accel, 3-axis gyro, 3-axis body accel]\n",
    "dataX_training = dstack(dataX_training) # To stack each of the loaded 3D arrays into a single 3D array\n",
    "\n",
    "dataY_trainingRaw = dataY_trainingRaw.values\n",
    "print(dataX_training.shape, dataY_trainingRaw.shape)\n",
    "#file name of test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5945, 128, 9) (5945, 5) (2410, 128, 9) (2410, 5)\n"
     ]
    }
   ],
   "source": [
    "filesname_test = list()\n",
    "filesname_test += ['total_acc_x_test.txt', 'total_acc_y_test.txt', 'total_acc_z_test.txt']\n",
    "filesname_test += ['body_acc_x_test.txt', 'body_acc_x_test.txt', 'body_acc_x_test.txt']\n",
    "filesname_test += ['body_gyro_x_test.txt', 'body_gyro_y_test.txt', 'body_gyro_z_test.txt']\n",
    "dataY_testingRaw = pd.read_csv('Dataset/test/y_test.txt',header=None, delim_whitespace=True)\n",
    "index2 = dataY_testingRaw[dataY_testingRaw[0] == 6].index\n",
    "# indexStandingT = dataY_testingRaw[dataY_testingRaw[0] == 5].index\n",
    "# indexSittingT = dataY_testingRaw[dataY_testingRaw[0] == 4].index\n",
    "# #print(index1.shape)\n",
    "# dataY_testingRaw[indexStandingT,:] = 4;\n",
    "dataY_testingRaw.drop(index= index2, inplace = True)\n",
    "dataY_testingRaw.replace(5,4,inplace = True)\n",
    "\n",
    "#print(index2.shape)\n",
    "dataX_testing = list()\n",
    "for name in filesname_test:\n",
    "    frame = pd.read_csv('Dataset/test/Inertial Signals/' + name,header=None, delim_whitespace=True) # load a single file\n",
    "    frame.drop(index= index2, inplace = True)\n",
    "    dataX_testing.append(frame.values) # store data as a numpy array\n",
    "# dataX_training is a 3D numpy array (samples[number of raws is any file), time steps(number of readings in a single window ex: 128)\n",
    "#                               ,features[nine features 3-axis accel, 3-axis gyro, 3-axis body accel]\n",
    "dataX_testing = dstack(dataX_testing) # To stack each of the loaded 3D arrays into a single 3D array\n",
    "dataY_testingRaw = dataY_testingRaw.values\n",
    "#print(dataX_testing.shape, dataY_testingRaw.shape)\n",
    "dataY_training = dataY_trainingRaw - 1\n",
    "dataY_testing = dataY_testingRaw - 1\n",
    "dataY_training = tf.keras.utils.to_categorical(dataY_training, num_classes = 5)\n",
    "dataY_testing = tf.keras.utils.to_categorical(dataY_testing, num_classes = 5)\n",
    "print(dataX_training.shape, dataY_training.shape,dataX_testing.shape, dataY_testing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1598, 128, 9) (1598, 1)\n",
      "(1598, 128, 9) (1598, 5)\n"
     ]
    }
   ],
   "source": [
    "filesname_serhat = list()\n",
    "filesname_serhat += ['total_acc_x.csv', 'total_acc_y.csv', 'total_acc_z.csv']\n",
    "filesname_serhat += ['body_acc_x.csv', 'body_acc_x.csv', 'body_acc_x.csv']\n",
    "filesname_serhat += ['body_gyro_x.csv', 'body_gyro_y.csv', 'body_gyro_z.csv']\n",
    "# indexStandingS = dataY_testingRaw[dataY_testingRaw[0] == 5].index\n",
    "# indexSittingS = dataY_testingRaw[dataY_testingRaw[0] == 4].index\n",
    "# #print(index1.shape)\n",
    "# dataY_testingRaw[indexStandingS,:] = 4;\n",
    "dataX_serhat = list()\n",
    "for name in filesname_serhat:\n",
    "    frame = pd.read_csv('New Data/After processing/' + name, header=None) # load a single file\n",
    "    dataX_serhat.append(frame.values) # store data as a numpy array\n",
    "dataX_serhat = dstack(dataX_serhat)\n",
    "dataY_serhat = pd.read_csv('New Data/After processing/labels.csv',header=None)\n",
    "dataY_serhat.replace(6,5,inplace = True)\n",
    "dataY_serhat = dataY_serhat.values\n",
    "print(dataX_serhat.shape, dataY_serhat.shape)\n",
    "dataY_serhat = dataY_serhat - 1\n",
    "dataY_serhat_T = tf.keras.utils.to_categorical(dataY_serhat, num_classes = 5)\n",
    "print(dataX_serhat.shape, dataY_serhat_T.shape)\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING_SITTING\",\n",
    "    \"RUNNING\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definetion of RNN model\n",
    "def RNN_model(dataX_training,dataY_training,dataX_testing,dataY_testing):\n",
    "    verbose, epochs, batch_size = 1, 1, 128\n",
    "    numTimestep, numFeatures, numOutputs = dataX_training.shape[1], dataX_training.shape[2], dataY_training.shape[1] \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(128,input_shape= (numTimestep, numFeatures)))\n",
    "    #model.add(tf.keras.layers.LSTM(64))return_sequences=True ,\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(numOutputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit (dataX_training, dataY_training, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    _, accuracy = model.evaluate(dataX_testing, dataY_testing, batch_size=batch_size, verbose=0)\n",
    "    return accuracy, model\n",
    "def RNN_model_V2 (dataX_training,dataY_training,dataX_testing,dataY_testing):\n",
    "    verbose, epochs, batch_size = 1, 50, 128\n",
    "    numTimestep, numFeatures, numOutputs = dataX_training.shape[1], dataX_training.shape[2], dataY_training.shape[1]\n",
    "    N_CLASSES = dataY_training.shape[1]\n",
    "    N_HIDDEN_UNITS = 200\n",
    "    L2 = 0.000001\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(N_HIDDEN_UNITS,return_sequences=True,input_shape= (numTimestep, numFeatures),\n",
    "                                   kernel_initializer='orthogonal', kernel_regularizer=l2(L2), recurrent_regularizer=l2(L2),\n",
    "                                   bias_regularizer=l2(L2), name=\"LSTM_1\"))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Flatten(name='Flatten'))\n",
    "    model.add(tf.keras.layers.Dense(N_HIDDEN_UNITS, activation='relu', kernel_regularizer=l2(L2), bias_regularizer=l2(L2), name=\"Dense_1\"))\n",
    "    model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax', kernel_regularizer=l2(L2), bias_regularizer=l2(L2), name=\"Dense_2\"))\n",
    "    model.summary()\n",
    "    opt = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    BATCH_SIZE = 128\n",
    "    N_EPOCHS = 50\n",
    "    log_dir = \"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=BATCH_SIZE, epochs=N_EPOCHS,\n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[tensorboard_callback])\n",
    "    _, accuracy = model.evaluate(dataX_testing, dataY_testing, batch_size=BATCH_SIZE, verbose=0)\n",
    "    return accuracy, model\n",
    "def buildLstmLayer(inputs, num_layers, num_units):\n",
    "  \"\"\"Build the lstm layer.\n",
    "\n",
    "  Args:\n",
    "    inputs: The input data.\n",
    "    num_layers: How many LSTM layers do we want.\n",
    "    num_units: The unmber of hidden units in the LSTM cell.\n",
    "  \"\"\"\n",
    "  lstm_cells = []\n",
    "  for i in range(num_layers):\n",
    "    lstm_cells.append(\n",
    "        tf.lite.experimental.nn.TFLiteLSTMCell(\n",
    "            num_units, forget_bias=0, name='rnn{}'.format(i)))\n",
    "  lstm_layers = tf.keras.layers.StackedRNNCells(lstm_cells)\n",
    "  # Assume the input is sized as [batch, time, input_size], then we're going\n",
    "  # to transpose to be time-majored.\n",
    "  transposed_inputs = tf.transpose(\n",
    "      inputs, perm=[1, 0, 2])\n",
    "  outputs, _ = tf.lite.experimental.nn.dynamic_rnn(\n",
    "      lstm_layers,\n",
    "      transposed_inputs,\n",
    "      dtype='float32',\n",
    "      time_major=True)\n",
    "  unstacked_outputs = tf.unstack(outputs, axis=0)\n",
    "  return unstacked_outputs[-1]\n",
    "def RNN_model_V3 (dataX_training,dataY_training,dataX_testing,dataY_testing):\n",
    "    verbose, epochs, batch_size = 1, 15, 128\n",
    "    numTimestep, numFeatures, numOutputs = dataX_training.shape[1], dataX_training.shape[2], dataY_training.shape[1]\n",
    "    N_CLASSES = dataY_training.shape[1]\n",
    "    N_HIDDEN_UNITS = 200\n",
    "    L2 = 0.000001\n",
    "    tf.reset_default_graph()\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Input(shape=(numTimestep, numFeatures), name='input'),\n",
    "      tf.keras.layers.Lambda(buildLstmLayer, arguments={'num_layers' : 2, 'num_units' : N_HIDDEN_UNITS}),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(N_HIDDEN_UNITS, activation='relu', kernel_regularizer=l2(L2), bias_regularizer=l2(L2), name=\"Dense_1\"),\n",
    "      tf.keras.layers.Dense(numOutputs, activation=tf.nn.softmax,kernel_regularizer=l2(L2), bias_regularizer=l2(L2), name='output')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    log_dir = \"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[tensorboard_callback],verbose =verbose)\n",
    "    _, accuracy = model.evaluate(dataX_testing, dataY_testing, batch_size=batch_size, verbose=0)\n",
    "    return accuracy, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9953, 128, 9)\n",
      "(9953, 128, 12)\n"
     ]
    }
   ],
   "source": [
    "totalResultAccuracy = list()\n",
    "inputX = np.concatenate((dataX_training ,dataX_serhat)) # serhat data + HAPT data\n",
    "inputY = np.concatenate((dataY_training ,dataY_serhat_T))\n",
    "inputX = np.concatenate((dataX_testing ,inputX))\n",
    "inputY = np.concatenate((dataY_testing ,inputY))\n",
    "print(inputX.shape)\n",
    "def rms_feature(input_data):\n",
    "    newFeature = input_data\n",
    "    RMSA = np.sqrt(np.square(input_data[:,:,0]) + np.square(input_data[:,:,1]) + np.square(input_data[:,:,2]))\n",
    "    newFeature = np.insert(newFeature,9,RMSA,axis=2)\n",
    "    RMSL = np.sqrt(np.square(input_data[:,:,3]) + np.square(input_data[:,:,4]) + np.square(input_data[:,:,5]))\n",
    "    newFeature = np.insert(newFeature,10,RMSL,axis=2)\n",
    "    RMSG = np.sqrt(np.square(input_data[:,:,6]) + np.square(input_data[:,:,7]) + np.square(input_data[:,:,8]))\n",
    "    newFeature = np.insert(newFeature,11,RMSG,axis=2)\n",
    "    return newFeature\n",
    "inputX = rms_feature(inputX)\n",
    "print(inputX.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputX, inputY, test_size=0.33, random_state=7)\n",
    "_, X_pred, _, y_pred = train_test_split(X_test, y_test, test_size=0.2, random_state=7)\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_conf(normalised_confusion_matrix):\n",
    "    width = 8\n",
    "    height = 6\n",
    "    plt.figure(figsize=(width, height))\n",
    "    sn.set(font_scale=0.8) # for label size\n",
    "    sn.heatmap(normalised_confusion_matrix, annot=True, annot_kws={\"size\": 8},xticklabels =LABELS , yticklabels=LABELS,cmap='inferno') # font size\n",
    "    plt.title(\"Confusion matrix\",fontsize = 'x-large')\n",
    "    #tick_marks = np.arange(6)\n",
    "    #plt.xticks(tick_marks, LABELS, rotation=90,fontsize = 'x-small' )\n",
    "    #plt.yticks(tick_marks, LABELS,  rotation='horizontal', fontsize = 'x-small')\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize = 'x-large')\n",
    "    plt.xlabel('Predicted label', fontsize = 'x-large')\n",
    "    plt.show()\n",
    "def outputmodel():\n",
    "    from tensorflow.keras import backend as K\n",
    "    from tensorflow.python.tools import freeze_graph\n",
    "    from tensorflow.python.tools import optimize_for_inference_lib\n",
    "    input_node_names= [\"LSTM_1_input\"]\n",
    "    output_node_name = \"Dense_2/Softmax\"\n",
    "    MODEL_NAME = \"HAR\"\n",
    "\n",
    "    tf.train.write_graph(K.get_session().graph_def, 'models', \\\n",
    "                         MODEL_NAME + '_graph.pbtxt')\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(K.get_session(), 'models/' + MODEL_NAME + '.chkp')\n",
    "    \n",
    "    freeze_graph.freeze_graph('models/' + MODEL_NAME + '_graph.pbtxt', None, \\\n",
    "        False, 'models/' + MODEL_NAME + '.chkp', output_node_name, \\\n",
    "        \"save/restore_all\", \"save/Const:0\", \\\n",
    "    'models/frozen_' + MODEL_NAME + '.pb', True, \"\")\n",
    "\n",
    "def tflite_SaveModel():\n",
    "    sess = tf.keras.backend.get_session()\n",
    "    input_tensor = sess.graph.get_tensor_by_name('input:0')\n",
    "    output_tensor = sess.graph.get_tensor_by_name('output/Softmax:0')\n",
    "    converter = tf.lite.TFLiteConverter.from_session(\n",
    "        sess, [input_tensor], [output_tensor])\n",
    "    tflite = converter.convert()\n",
    "    print('Model converted successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6668, 128, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3285, 128, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(657, 128, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Displaying Total Acceleration_X of prediction Data:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.995660</td>\n",
       "      <td>1.008585</td>\n",
       "      <td>1.019109</td>\n",
       "      <td>1.015023</td>\n",
       "      <td>1.007409</td>\n",
       "      <td>1.014460</td>\n",
       "      <td>1.020951</td>\n",
       "      <td>1.024690</td>\n",
       "      <td>1.024579</td>\n",
       "      <td>1.014256</td>\n",
       "      <td>...</td>\n",
       "      <td>1.008740</td>\n",
       "      <td>1.005485</td>\n",
       "      <td>1.005986</td>\n",
       "      <td>1.012847</td>\n",
       "      <td>1.022996</td>\n",
       "      <td>1.017718</td>\n",
       "      <td>1.011562</td>\n",
       "      <td>1.018086</td>\n",
       "      <td>1.021002</td>\n",
       "      <td>1.023328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.839599</td>\n",
       "      <td>0.860402</td>\n",
       "      <td>0.976567</td>\n",
       "      <td>1.519542</td>\n",
       "      <td>1.987117</td>\n",
       "      <td>1.877374</td>\n",
       "      <td>1.798442</td>\n",
       "      <td>1.877583</td>\n",
       "      <td>1.648914</td>\n",
       "      <td>1.462424</td>\n",
       "      <td>...</td>\n",
       "      <td>1.854027</td>\n",
       "      <td>1.566539</td>\n",
       "      <td>1.176818</td>\n",
       "      <td>1.119778</td>\n",
       "      <td>1.125409</td>\n",
       "      <td>0.983631</td>\n",
       "      <td>0.948585</td>\n",
       "      <td>0.878007</td>\n",
       "      <td>0.731541</td>\n",
       "      <td>0.676386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.919773</td>\n",
       "      <td>0.908361</td>\n",
       "      <td>0.906621</td>\n",
       "      <td>0.912637</td>\n",
       "      <td>0.915176</td>\n",
       "      <td>0.916087</td>\n",
       "      <td>0.919569</td>\n",
       "      <td>0.923227</td>\n",
       "      <td>0.924299</td>\n",
       "      <td>0.919345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945968</td>\n",
       "      <td>0.957062</td>\n",
       "      <td>0.958839</td>\n",
       "      <td>0.950312</td>\n",
       "      <td>0.944105</td>\n",
       "      <td>0.938900</td>\n",
       "      <td>0.937242</td>\n",
       "      <td>0.939127</td>\n",
       "      <td>0.940604</td>\n",
       "      <td>0.949762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.939556</td>\n",
       "      <td>0.938853</td>\n",
       "      <td>0.942722</td>\n",
       "      <td>0.944009</td>\n",
       "      <td>0.948057</td>\n",
       "      <td>0.946499</td>\n",
       "      <td>0.941201</td>\n",
       "      <td>0.944089</td>\n",
       "      <td>0.946197</td>\n",
       "      <td>0.946105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.934095</td>\n",
       "      <td>0.934312</td>\n",
       "      <td>0.935589</td>\n",
       "      <td>0.936635</td>\n",
       "      <td>0.934977</td>\n",
       "      <td>0.935006</td>\n",
       "      <td>0.938653</td>\n",
       "      <td>0.942271</td>\n",
       "      <td>0.942299</td>\n",
       "      <td>0.940997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.156841</td>\n",
       "      <td>0.941408</td>\n",
       "      <td>0.761534</td>\n",
       "      <td>0.750770</td>\n",
       "      <td>0.551307</td>\n",
       "      <td>0.650904</td>\n",
       "      <td>0.719152</td>\n",
       "      <td>0.646910</td>\n",
       "      <td>0.756342</td>\n",
       "      <td>0.690230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.825817</td>\n",
       "      <td>0.824232</td>\n",
       "      <td>0.809123</td>\n",
       "      <td>0.800251</td>\n",
       "      <td>0.760133</td>\n",
       "      <td>0.749567</td>\n",
       "      <td>0.751689</td>\n",
       "      <td>0.754406</td>\n",
       "      <td>0.787006</td>\n",
       "      <td>0.790111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.016016</td>\n",
       "      <td>1.015524</td>\n",
       "      <td>1.014270</td>\n",
       "      <td>1.013276</td>\n",
       "      <td>1.013223</td>\n",
       "      <td>1.015372</td>\n",
       "      <td>1.016328</td>\n",
       "      <td>1.013430</td>\n",
       "      <td>1.013589</td>\n",
       "      <td>1.014562</td>\n",
       "      <td>...</td>\n",
       "      <td>1.012706</td>\n",
       "      <td>1.014327</td>\n",
       "      <td>1.015885</td>\n",
       "      <td>1.014028</td>\n",
       "      <td>1.012606</td>\n",
       "      <td>1.012284</td>\n",
       "      <td>1.012461</td>\n",
       "      <td>1.012734</td>\n",
       "      <td>1.012059</td>\n",
       "      <td>1.011943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.959933</td>\n",
       "      <td>0.959587</td>\n",
       "      <td>0.960946</td>\n",
       "      <td>0.962333</td>\n",
       "      <td>0.962177</td>\n",
       "      <td>0.959870</td>\n",
       "      <td>0.957353</td>\n",
       "      <td>0.958108</td>\n",
       "      <td>0.958992</td>\n",
       "      <td>0.958083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957268</td>\n",
       "      <td>0.956913</td>\n",
       "      <td>0.960429</td>\n",
       "      <td>0.963577</td>\n",
       "      <td>0.961915</td>\n",
       "      <td>0.950409</td>\n",
       "      <td>0.939068</td>\n",
       "      <td>0.944999</td>\n",
       "      <td>0.953941</td>\n",
       "      <td>0.955494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.870173</td>\n",
       "      <td>-0.057087</td>\n",
       "      <td>-0.165117</td>\n",
       "      <td>0.852937</td>\n",
       "      <td>1.162952</td>\n",
       "      <td>1.160668</td>\n",
       "      <td>0.796150</td>\n",
       "      <td>0.394063</td>\n",
       "      <td>0.409222</td>\n",
       "      <td>0.388134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764358</td>\n",
       "      <td>0.854255</td>\n",
       "      <td>0.930695</td>\n",
       "      <td>0.981442</td>\n",
       "      <td>0.985027</td>\n",
       "      <td>1.084287</td>\n",
       "      <td>1.136993</td>\n",
       "      <td>1.270689</td>\n",
       "      <td>1.439320</td>\n",
       "      <td>1.558629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.996397</td>\n",
       "      <td>0.994984</td>\n",
       "      <td>0.996098</td>\n",
       "      <td>0.996256</td>\n",
       "      <td>0.996137</td>\n",
       "      <td>1.001864</td>\n",
       "      <td>1.006233</td>\n",
       "      <td>1.008223</td>\n",
       "      <td>1.010376</td>\n",
       "      <td>1.012219</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011200</td>\n",
       "      <td>1.008546</td>\n",
       "      <td>1.011927</td>\n",
       "      <td>1.013540</td>\n",
       "      <td>1.011792</td>\n",
       "      <td>1.011466</td>\n",
       "      <td>1.010178</td>\n",
       "      <td>1.008320</td>\n",
       "      <td>1.006326</td>\n",
       "      <td>1.002159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.003757</td>\n",
       "      <td>0.927225</td>\n",
       "      <td>0.809709</td>\n",
       "      <td>0.776457</td>\n",
       "      <td>0.809348</td>\n",
       "      <td>0.880805</td>\n",
       "      <td>0.977979</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.959382</td>\n",
       "      <td>0.944077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867365</td>\n",
       "      <td>0.840576</td>\n",
       "      <td>0.928745</td>\n",
       "      <td>1.006269</td>\n",
       "      <td>0.870111</td>\n",
       "      <td>0.827547</td>\n",
       "      <td>0.818132</td>\n",
       "      <td>0.860951</td>\n",
       "      <td>0.977804</td>\n",
       "      <td>0.960709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.995660  1.008585  1.019109  1.015023  1.007409  1.014460  1.020951   \n",
       "1  0.839599  0.860402  0.976567  1.519542  1.987117  1.877374  1.798442   \n",
       "2  0.919773  0.908361  0.906621  0.912637  0.915176  0.916087  0.919569   \n",
       "3  0.939556  0.938853  0.942722  0.944009  0.948057  0.946499  0.941201   \n",
       "4  1.156841  0.941408  0.761534  0.750770  0.551307  0.650904  0.719152   \n",
       "5  1.016016  1.015524  1.014270  1.013276  1.013223  1.015372  1.016328   \n",
       "6  0.959933  0.959587  0.960946  0.962333  0.962177  0.959870  0.957353   \n",
       "7  1.870173 -0.057087 -0.165117  0.852937  1.162952  1.160668  0.796150   \n",
       "8  0.996397  0.994984  0.996098  0.996256  0.996137  1.001864  1.006233   \n",
       "9  1.003757  0.927225  0.809709  0.776457  0.809348  0.880805  0.977979   \n",
       "\n",
       "        7         8         9    ...       118       119       120       121  \\\n",
       "0  1.024690  1.024579  1.014256  ...  1.008740  1.005485  1.005986  1.012847   \n",
       "1  1.877583  1.648914  1.462424  ...  1.854027  1.566539  1.176818  1.119778   \n",
       "2  0.923227  0.924299  0.919345  ...  0.945968  0.957062  0.958839  0.950312   \n",
       "3  0.944089  0.946197  0.946105  ...  0.934095  0.934312  0.935589  0.936635   \n",
       "4  0.646910  0.756342  0.690230  ...  0.825817  0.824232  0.809123  0.800251   \n",
       "5  1.013430  1.013589  1.014562  ...  1.012706  1.014327  1.015885  1.014028   \n",
       "6  0.958108  0.958992  0.958083  ...  0.957268  0.956913  0.960429  0.963577   \n",
       "7  0.394063  0.409222  0.388134  ...  0.764358  0.854255  0.930695  0.981442   \n",
       "8  1.008223  1.010376  1.012219  ...  1.011200  1.008546  1.011927  1.013540   \n",
       "9  0.992982  0.959382  0.944077  ...  0.867365  0.840576  0.928745  1.006269   \n",
       "\n",
       "        122       123       124       125       126       127  \n",
       "0  1.022996  1.017718  1.011562  1.018086  1.021002  1.023328  \n",
       "1  1.125409  0.983631  0.948585  0.878007  0.731541  0.676386  \n",
       "2  0.944105  0.938900  0.937242  0.939127  0.940604  0.949762  \n",
       "3  0.934977  0.935006  0.938653  0.942271  0.942299  0.940997  \n",
       "4  0.760133  0.749567  0.751689  0.754406  0.787006  0.790111  \n",
       "5  1.012606  1.012284  1.012461  1.012734  1.012059  1.011943  \n",
       "6  0.961915  0.950409  0.939068  0.944999  0.953941  0.955494  \n",
       "7  0.985027  1.084287  1.136993  1.270689  1.439320  1.558629  \n",
       "8  1.011792  1.011466  1.010178  1.008320  1.006326  1.002159  \n",
       "9  0.870111  0.827547  0.818132  0.860951  0.977804  0.960709  \n",
       "\n",
       "[10 rows x 128 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "X_pred = X_pred.astype(np.float32)\n",
    "view1 = pd.DataFrame(X_pred[:,:,0])\n",
    "display(X_train.shape)\n",
    "display(X_test.shape)\n",
    "display(X_pred.shape)\n",
    "display(\"Displaying Total Acceleration_X of prediction Data:\")\n",
    "display(view1.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(657, 128, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for features in range(0,np.size(X_pred,2),1):\n",
    "    arr = pd.DataFrame(X_pred[:,:,features])\n",
    "    path2=\"pred\\\\pred_\"+str(features)+\".csv\"\n",
    "    arr.to_csv(path_or_buf=path2, na_rep='NaN', columns=None, header=False, index=False, \n",
    "                      mode='w', encoding='utf-8',line_terminator='\\n',)    \n",
    "np.savetxt(\"pred\\\\pred_labels.csv\",y_pred, delimiter=\",\",fmt ='%d')        \n",
    "display(X_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'input',\n",
       "  'index': 5,\n",
       "  'shape': array([  1, 128,  12]),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0)}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'output/Softmax',\n",
       "  'index': 142,\n",
       "  'shape': array([1, 5]),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0)}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"RNN_LSTM.tflite\")\n",
    "try:\n",
    "  interpreter.allocate_tensors()\n",
    "except ValueError:\n",
    "  assert False\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "display(input_details, output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Lite Evaluation result is 0.9558599695585996\n"
     ]
    }
   ],
   "source": [
    "MINI_BATCH_SIZE = 1\n",
    "correct_case = 0\n",
    "for i in range(len(X_pred)):\n",
    "  input_index = (interpreter.get_input_details()[0]['index'])\n",
    "  interpreter.set_tensor(input_index, X_pred[i * MINI_BATCH_SIZE: (i + 1) * MINI_BATCH_SIZE])\n",
    "  interpreter.invoke()\n",
    "  output_index = (interpreter.get_output_details()[0]['index'])\n",
    "  result = interpreter.get_tensor(output_index)\n",
    "  # Reset all variables so it will not pollute other inferences.\n",
    "  interpreter.reset_all_variables()\n",
    "  # Evaluate.\n",
    "  prediction = np.argmax(result)\n",
    "  if prediction == y_pred[i]:\n",
    "    correct_case += 1\n",
    "\n",
    "print('TensorFlow Lite Evaluation result is {}'.format(correct_case * 1.0 / len(X_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
